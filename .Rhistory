)
View(fli_co)
ggplot(data = diamonds) +
geom_bar(mappping = aes(x = cut))
library(ggplot2)
ggplot(data = diamonds) +
geom_bar(mappping = aes(x = cut))
ggplot(data = diamonds) +
geom_bar(mapping = aes(x = cut))
ggplot(data = diamonds) +
geom_bar(mapping = aes(x = cut, alpha = 0.5, fill = cut, size = 5, weight = 2))
library(ggplot2)
ggplot(data = diamonds) +
geom_bar(mapping = aes(x = cut, alpha = 0.5, fill = cut, size = 2, weight = 2))
library(ggplot2)
ggplot(data = diamonds) +
geom_bar(mapping = aes(x = cut, alpha = 0.5, fill = cut, size = 2))
library(ggplot2)
ggplot(data = diamonds) +
geom_bar(mapping = aes(x = cut, alpha = 1, fill = cut, size = 2))
library(ggplot2)
ggplot(data = diamonds) +
geom_bar(mapping = aes(x = cut,fill = cut))
ggplot(data = diamonds) +
geom_histogram(aes(x = carat))
ggplot(data = diamonds) +
geom_histogram(aes(x = carat), binwidth = 1)
ggplot(data = diamonds) +
geom_histogram(aes(x = carat), binwidth = 0.01)
ggplot(data = economics) +
geom_point(aes(x = date, y = unemploy))
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
ggplot(data = diamonds) +
geom_point(mapping = aes(x = carat, y = price)) +
geom_smooth(mapping = aes(x = carat, y = price))
cor(diamonds$carat, diamonds$price)
library(tibble)
?tibble
as_data_frame(iris)
class(as.data.frame(tbl_df(iris)))
library(readr)
library(tidyr)
library(dplyr)
libary(DSR)
install.packages("DSR")
libary(DSR)
install.packages("DSR")
library(nycflights13)
library(dplyr)
library(stringr)
library(ggplot2)
library(lubridate)
install.packages("lubridate"")
install.packages("lubridate")
library(lubridate)
datetimes <- flights %>%
mutate(departure = make_datetime(year = year, month = month, day = day,
hour = hour, min = minute))
head(datetimes)
(datetimes <- flights %>%
mutate(departure = make_datetime(year = year, month = month, day = day,
hour = hour, min = minute)))
(datetimes <- datetimes %>%
mutate(arrival = make_datetime(year = year, month = month, day = day,
hour = str_sub(arr_time, end = -3),
min = str_sub(arr_time, start = -2))) %>%
filter(!is.na(departure), !is.na(arrival)) %>%
select(departure, arrival, dep_delay, arr_delay, carrier, tailnum,
flight, origin, dest, air_time, distance))
trial <- ymd("20170131")
unclass(trial)
class(trial)
library(ggplot2)
library(dplyr)
library(mgcv)
library(splines)
library(broom)
heights
head(heights)
?mgcv
?splines
?broom
library(ggplot2)
library(ggplot2)
getwd()
library(ggplot2)
knit_with_parameters('~/Desktop/manhatta/manhattanestates.Rmd')
library(knitr)
knit_with_parameters('~/Desktop/manhatta/manhattanestates.Rmd')
rm(list=ls())
ls()
R.version()
?version
R.Version()
library(h2o)
localH2O = h2o.init(nthreads=-1)
localH2O = h2o.init(nthreads=-1)
demo(h2o.kmeans)
demo(h2o.kmeans)
weight.i <- 0.01*matrix(rnorm(layer.size(i)*layer.size(i+1)),
nrow=layer.size(i),
ncol=layer.size(i+1))
input <- matrix(1:4, nrow=2, ncol=2)
weights <- matrix(1:6, nrow=2, ncol=3)
bias <- matrix(1:3, nrow=1, ncol=3)
input %*% weights + bias
s1 <- input %*% weights + matrix(rep(bias, each=2), ncol=3)
s2 <- sweep(input %*% weights ,2, bias, '+')
?sweep
?"%*%"
A <- array(1:24, dim = 4:2)
A
A
A
A <- array(1:5, dim = 4:2)
A
A <- array(1:5)
A
A <- array(1:5, dim = 2:2)
A
A <- array(1:5, dim = 3:2)
A
A <- array(1:5, dim = 3:1)
A
s2 <- sweep(input %*% weights ,2, bias, '+')
all.equal(s1, s2)
summary(iris)
summary(iris)
str(ir.model)
str(iris.model)
s1
max(0, s1)
pmax(0, s1)
pmax(s1, 0)
predict.dnn <- function(model, data = X.test) {
# new data, transfer to matrix
new.data <- data.matrix(data)
# Feed Forwad
hidden.layer <- sweep(new.data %*% model$W1 ,2, model$b1, '+')
# neurons : Rectified Linear
hidden.layer <- pmax(hidden.layer, 0)
score <- sweep(hidden.layer %*% model$W2, 2, model$b2, '+')
# Loss Function: softmax
score.exp <- exp(score)
probs <-sweep(score.exp, 1, rowSums(score.exp), '/')
# select max possiblity
labels.predicted <- max.col(probs)
return(labels.predicted)
}
train.dnn <- function(x, y, traindata=data, testdata=NULL,
model = NULL,
# set hidden layers and neurons
# currently, only support 1 hidden layer
hidden=c(6),
# max iteration steps
maxit=2000,
# delta loss
abstol=1e-2,
# learning rate
lr = 1e-2,
# regularization rate
reg = 1e-3,
# show results every 'display' step
display = 100,
random.seed = 1)
{
# to make the case reproducible.
set.seed(random.seed)
# total number of training set
N <- nrow(traindata)
# extract the data and label
# don't need atribute
X <- unname(data.matrix(traindata[,x]))
# correct categories represented by integer
Y <- traindata[,y]
if(is.factor(Y)) { Y <- as.integer(Y) }
# create index for both row and col
# create index for both row and col
Y.len   <- length(unique(Y))
Y.set   <- sort(unique(Y))
Y.index <- cbind(1:N, match(Y, Y.set))
# create model or get model from parameter
if(is.null(model)) {
# number of input features
D <- ncol(X)
# number of categories for classification
K <- length(unique(Y))
H <-  hidden
# create and init weights and bias
W1 <- 0.01*matrix(rnorm(D*H), nrow=D, ncol=H)
b1 <- matrix(0, nrow=1, ncol=H)
W2 <- 0.01*matrix(rnorm(H*K), nrow=H, ncol=K)
b2 <- matrix(0, nrow=1, ncol=K)
} else {
D  <- model$D
K  <- model$K
H  <- model$H
W1 <- model$W1
b1 <- model$b1
W2 <- model$W2
b2 <- model$b2
}
# use all train data to update weights since it's a small dataset
batchsize <- N
# init loss to a very big value
loss <- 100000
# Training the network
i <- 0
while(i < maxit && loss > abstol ) {
# iteration index
i <- i +1
# forward ....
# 1 indicate row, 2 indicate col
hidden.layer <- sweep(X %*% W1 ,2, b1, '+')
# neurons : ReLU
hidden.layer <- pmax(hidden.layer, 0)
score <- sweep(hidden.layer %*% W2, 2, b2, '+')
# softmax
score.exp <- exp(score)
# debug
probs <- score.exp/rowSums(score.exp)
# compute the loss
corect.logprobs <- -log(probs[Y.index])
data.loss  <- sum(corect.logprobs)/batchsize
reg.loss   <- 0.5*reg* (sum(W1*W1) + sum(W2*W2))
loss <- data.loss + reg.loss
# display results and update model
if( i %% display == 0) {
if(!is.null(testdata)) {
model <- list( D = D,
H = H,
K = K,
# weights and bias
W1 = W1,
b1 = b1,
W2 = W2,
b2 = b2)
labs <- predict.dnn(model, testdata[,-y])
accuracy <- mean(as.integer(testdata[,y]) == Y.set[labs])
cat(i, loss, accuracy, "\n")
} else {
cat(i, loss, "\n")
}
}
# backward ....
dscores <- probs
dscores[Y.index] <- dscores[Y.index] -1
dscores <- dscores / batchsize
dW2 <- t(hidden.layer) %*% dscores
db2 <- colSums(dscores)
dhidden <- dscores %*% t(W2)
dhidden[hidden.layer <= 0] <- 0
dW1 <- t(X) %*% dhidden
db1 <- colSums(dhidden)
# update ....
dW2 <- dW2 + reg*W2
dW1 <- dW1  + reg*W1
W1 <- W1 - lr * dW1
b1 <- b1 - lr * db1
W2 <- W2 - lr * dW2
b2 <- b2 - lr * db2
}
# final results
# creat list to store learned parameters
# you can add more parameters for debug and visualization
# such as residuals, fitted.values ...
model <- list( D = D,
H = H,
K = K,
# weights and bias
W1= W1,
b1= b1,
W2= W2,
b2= b2)
return(model)
}
set.seed(1)
summary(iris)
plot(iris)
samp <- c(sample(1:50,25), sample(51:100,25), sample(101:150,25))
ir.model <- train.dnn(x=1:4, y=5, traindata=iris[samp,], testdata=iris[-samp,], hidden=10, maxit=2000, display=50)
labels.dnn <- predict.dnn(ir.model, iris[-samp, -5])
table(iris[-samp,5], labels.dnn)
mean(as.integer(iris[-samp, 5]) == labels.dnn)
library(nnet)
ird <- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]),
species = factor(c(rep("s",50), rep("c", 50), rep("v", 50))))
ir.nn2 <- nnet(species ~ ., data = ird, subset = samp, size = 6, rang = 0.1,
decay = 1e-2, maxit = 2000)
labels.nnet <- predict(ir.nn2, ird[-samp,], type="class")
table(ird$species[-samp], labels.nnet)
mean(ird$species[-samp] == labels.nnet)
data1 <- ("i loss accuracy
50 1.098421 0.3333333
100 1.098021 0.3333333
150 1.096843 0.3333333
200 1.093393 0.3333333
250 1.084069 0.3333333
300 1.063278 0.3333333
350 1.027273 0.3333333
400 0.9707605 0.64
450 0.8996356 0.6666667
500 0.8335469 0.6666667
550 0.7662386 0.6666667
600 0.6914156 0.6666667
650 0.6195753 0.68
700 0.5620381 0.68
750 0.5184008 0.7333333
800 0.4844815 0.84
850 0.4568258 0.8933333
900 0.4331083 0.92
950 0.4118948 0.9333333
1000 0.392368 0.96
1050 0.3740457 0.96
1100 0.3566594 0.96
1150 0.3400993 0.9866667
1200 0.3243276 0.9866667
1250 0.3093422 0.9866667
1300 0.2951787 0.9866667
1350 0.2818472 0.9866667
1400 0.2693641 0.9866667
1450 0.2577245 0.9866667
1500 0.2469068 0.9866667
1550 0.2368819 0.9866667
1600 0.2276124 0.9866667
1650 0.2190535 0.9866667
1700 0.2111565 0.9866667
1750 0.2038719 0.9866667
1800 0.1971507 0.9866667
1850 0.1909452 0.9866667
1900 0.1852105 0.9866667
1950 0.1799045 0.9866667
2000 0.1749881 0.9866667  ")
data.v <- read.table(text=data1, header=T)
par(mar=c(5.1, 4.1, 4.1, 4.1))
plot(x=data.v$i, y=data.v$loss, type="o", col="blue", pch=16,
main="IRIS loss and accuracy by 2-layers DNN",
ylim=c(0, 1.2),
xlab="",
ylab="",
axe =F)
box()
axis(1, at=seq(0,2000,by=200))
axis(4, at=seq(0,1.0,by=0.1))
axis(2, at=seq(0,1.2,by=0.1))
mtext("training step", 1, line=3)
mtext("loss of training set", 2, line=2.5)
mtext("accuracy of testing set", 4, line=2)
legend("bottomleft",
legend = c("loss", "accuracy"),
pch = c(16,1),
col = c("blue","red"),
lwd=c(1,1)
)
getwd()
shiny::runApp('jaea/shi')
runApp('jaea/shi')
getwd()
shiny::runApp('~/RProjects2016/jaea/shi')
library(plotly)
runApp('~/RProjects2016/jaea/shi')
shiny::runApp('~/RProjects2016/jaea/shi')
library(plotly)
runApp('~/RProjects2016/jaea/shi')
shiny::runApp('~/R_Projects/jaea/daichi')
getwd()
demo()
demo(package = .packages(all.available = TRUE))
demo(package = tidyr)
demo(package = "tidyr")
library(tidyr)
demo(package = tidyr)
demo(package = "tidyr")
demo(so-15668870 )
?demo
demo(so-15668870,tidyr)
demo(Data tidying: gather + separate + spread, tidyr)
demo(topic=Data tidying,package=tidyr)
demo(topic=so-15668870,package=tidyr)
demo(topic=gather,package=tidyr)
demo(lm.glm, package = "stats", ask = FALSE)
install.packages("glmnet")
library(glmnet)
unclass(cv.glmnet
unclass(cv.glmnet())
unclass(cv.glmnet
unclass(cv.glmnet)
?cv.glmnet
?ceiling
ls
ls()
getwd()
library(pryr)
library(ggplot2)
library(devtools)
install.packages("devtools")
devtools::install_github("hadley/lineprof")
object_size(1:10)
object_size(mean)
object_size(mtcars)
?sapply
sizes <- sapply(0:50, function(n) object_size(seq_len(n)))
plot(0:50, sizes, xlab = "Length", ylab = "Size (bytes)", type = "s")
plot(0:50, sizes, xlab = "Length", ylab = "Size (bytes)", type = "s")
shiny::runApp('shama')
library(ggvis)
install.packages("ggvis")
runApp('shama')
runApp('shama')
runApp('shama')
runApp('shama')
runApp('shama')
?dataset
summary(UCBAdmissions)
head(UCBAdmissions)
UCBAdmissions
summary(Titanic)
head(Titanic)
View(Titanic)
runApp('shama')
ggplot(data = Titanic)+
geom_point(stat="identity",mapping = aes(x= Titanic$class, y = Titanic$Freq,fill=Titanic$Age))
library(ggplot2)
ggplot(data = Titanic)+
geom_point(stat="identity",mapping = aes(x= Titanic$class, y = Titanic$Freq,fill=Titanic$Age))
class(Titanic)
titanic <- as.data.frame(Titanic)
class(titanic)
ggplot(data = titanic)+
geom_point(stat="identity",mapping = aes(x= titanic$class, y = titanic$Freq,fill=titanic$Age))
ggplot(data = titanic)+
geom_point(stat="identity",mapping = aes(x= titanic$class, y = titanic$Freq))
ggplot(data = titanic)+
geom_point(stat="identity",mapping = aes(x= class, y = Freq,fill=Age))
ggplot(data = titanic)+
geom_point(stat="identity",mapping = aes(x= class, y = Freq))
ggplot(data = titanic)+
geom_bar(stat="identity",mapping = aes(x= Freq))
ggplot(data = titanic)+
geom_bar(mapping = aes(x= Freq))
ggplot(data = titanic)+
geom_bar(mapping = aes(x= Freq, fill = Age))
ggplot(data = titanic)+
geom_bar(mapping = aes(x= Freq, fill = Survived))
ggplot(data = titanic)+
geom_bar(mapping = aes(x= Freq, fill = class))
ggplot(data = titanic)+
geom_bar(mapping = aes(x= Freq, fill = sex))
ggplot(data = titanic)+
geom_bar(mapping = aes(x= Freq, fill = Sex))
ggplot(data = titanic)+
geom_bar(mapping = aes(x= Freq, fill = Age))
geom_point(mapping = aes(x= Freq, fill = Age))
ggplot(data = titanic)+
geom_point(mapping = aes(x= Freq, y = Age))
ggplot(data = titanic)+
geom_point(mapping = aes(x= Freq, fill = Age))
ggplot(data = titanic)+
geom_bar(mapping = aes(x= Freq, fill = Age))
ggplot(data = titanic)+
geom_point(mapping = aes(x= Freq, y = Survived))
ggplot(data = titanic)+
geom_point(mapping = aes(x= Freq, y = Sex))
runApp('shama')
runApp('shama')
runApp('shama')
runApp('shama')
runApp('shama')
runApp('shama')
runApp('shama')
runApp('shama')
runApp('shama')
runApp('shama')
runApp('shama')
runApp('shama')
runApp('shama')
install.packages("e1071")
library(MASS)
library(e1071)
set.seed(2020)
Version()
?Version
Version()
RVersion()
R.Version()
R.Version()
?ecdf
?leadline
?readline
jio <- c("moving out of schpoo")
readline(jio)
?POSIXlt
?POSIXct
ls()
air13 <- read.csv("thesisVisuals/air13.csv")
bure = c(0,1,5,10,40)
air13$unAnnualExDoseRange <- cut(air13$unAnnualExtDose, breaks = bure)
air13$AnnualExDoseRange <- cut(air13$AnnualExtDose, breaks = bure)
R.Version()
getwd()
getwd()
setwd("/Users/ntabgoba/nltk1/")
list.files()
titles <- read.csv("titles_eng.csv")
dim(titles)
head(titles)
titles <- read.csv("titles_eng.csv",sep = "\t")
head(title)
head(titles)
View(titles)
list.files()
